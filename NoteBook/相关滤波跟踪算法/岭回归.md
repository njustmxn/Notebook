# 岭回归

通过检测实现跟踪（tracking-by-detection）的思想就是将目标跟踪的问题转化为前景与背景的二分类问题。对于给定的训练样本集$\{ ({{{\bf{x}}_i}},{y_i}),i = 1,2, \cdots ,l\}$，通过寻找正则风险最小化（regularized risk minimization）参数来训练分类器。通常情况下，其线性分类器的形式为:

$$f({{{\bf{x}}_i}}) = \left\langle {\bf{w},{{{\bf{x}}_i}}} \right\rangle + b\tag{1}$$
其中$\left\langle { \cdot , \cdot } \right\rangle$表示点积运算，${\bf{w}}$为待求参数向量， $b$为偏移常数，在实际应用中通常将 增广一个常数为1的项，从而将$b$划归为${\bf{w}}$的最后一个元素，故在表达式中可将其省略，简化为$f({{\bf{x}}_i}) = \left\langle {{\bf{w}},{{\bf{x}}_i}} \right\rangle$。正则风险最小化问题可以表示为

$$\mathop {\min }\limits_{\bf{w}} \sum\limits_{i = 1}^m {L\left( {{y_i},f({{\bf{x}}_i})} \right) + \lambda {{\left\| {\bf{w}} \right\|}^2}} \tag{2}$$

其中，$L( \cdot )$表示损失函数，$\lambda$是防止过拟合而设置的正则化参数。式(2)为一个通用框架，在SVM中，其损失函数采用合页损失$L\left( {{y_i},f({{\bf{x}}_i})} \right) = \max \left( {0,1 - {y_i}f({{\bf{x}}_i})} \right)$，而在正则化最小二乘法(regularized least squares, RLS)中，又被称为岭回归(ridge regression, RR)，损失函数则使用二次损失$L\left( {{y_i},f({{\bf{x}}_i})} \right) = {\left( {{y_i} - f({{\bf{x}}_i})} \right)^2}$。根据样本是否线性可分，可以分为以下两种情况。

## 线性岭回归
在样本线性可分的情况下，设${\bf{X}}$为n个具有m个特征的样本集，第i个样本表示为${{\bf{x}}_i}$， ${\bf{y}}$为对应${\bf{X}}$的n个样本类别标签，即${\bf{X}}$为n×m的矩阵， ${{\bf{x}}_i}$为1×m的行向量， ${\bf{y}}$为n×1的列向量，则${\bf{w}}$是m×1的列向量，满足${y_i} \approx f({{\bf{x}}_i}) = \left\langle {{\bf{w}},{{\bf{x}}_i}} \right\rangle  = {{\bf{x}}_i}{\bf{w}}$形式。对于正则风险最小化问题，令：

$$\displaylines{
  F({\bf{w}}) = \sum\limits_{i = 1}^n {{{\left( {{y_i} - f({{\bf{x}}_i})} \right)}^2} + \lambda {{\left\| {\bf{w}} \right\|}^2}}  \cr 
   = {({\bf{y}} - {\bf{Xw}})^T}({\bf{y}} - {\bf{Xw}}) + \lambda {{\bf{w}}^{\rm{T}}}{\bf{w}} \cr}$$
   
   欲使$F({\bf{w}})$值最小，仅需对${\bf{w}}$求导，并令其等于0，
   
   $$\displaylines{
  {{dF({\bf{w}})} \over {d{\bf{w}}}} = {({\bf{y}} - {\bf{Xw}})^{\rm{T}}}( - {\bf{X}}) + \lambda {{\bf{w}}^{\rm{T}}} \cr 
   = {{\bf{w}}^{\rm{T}}}({{\bf{X}}^{\rm{T}}}{\bf{X}} + \lambda I) - {{\bf{y}}^{\rm{T}}}{\bf{X}} \cr = 0 \cr} $$
   由此得到线性岭回归的解为：
   
   $${\bf{w}} = {({{\bf{X}}^{\rm{T}}}{\bf{X}} + \lambda {\bf{I}})^{ - 1}}{{\bf{X}}^{\rm{T}}}{\bf{y}}\tag{3}$$
   当样本为频域数据时，该解的形式为：
   
   $${\bf{w}} = {({{\bf{X}}^{\rm{H}}}{\bf{X}} + \lambda {\bf{I}})^{ - 1}}{{\bf{X}}^{\rm{H}}}{\bf{y}}\tag{4}$$
   其中上标${\left(  \cdot  \right)^{\rm{H}}}$表示Hermitian转置，即共轭转置，形式为${{\bf{X}}^{\rm{H}}} = {\left( {{{\bf{X}}^ * }} \right)^{\rm{T}}}$。
   
   
## 核岭回归
当样本线性不可分时，可以通过引入核函数的方式，将样本投影到高维特征空间，从而使用线性回归的手段寻找非线性样本的最优分类面，这一点与SVM是一致的。  
设映射函数为$\varphi ({\bf{x}})$，根据表示定理（Representer Theorem）[47]，该分类器的解可以表示为所有输入样本的线性组合，即：

$${\bf{w}} = \sum\limits_{i = 1}^n {{\alpha _i}\varphi ({{\bf{x}}_i})} $$
其中，${\bf{\alpha }}$为n×1的列向量，是正则化风险最小化问题在对偶空间中的待求参量，则对应样本的分类面为：

$$\displaylines{
  f({{\bf{x}}_j}) = \left\langle {{\bf{w}},\varphi {{\bf{x}}_j}} \right\rangle  \cr
   = \varphi ({{\bf{x}}_j})\sum\limits_{i = 1}^n {{\alpha _i}\varphi ({{\bf{x}}_i})}  \cr
   = \sum\limits_{i = 1}^n {{\alpha _i}} \left\langle {\varphi ({{\bf{x}}_i})\varphi ({{\bf{x}}_j})} \right\rangle  \cr} $$
   令${\bf{K}}$表示n×n的核函数矩阵，其中 ${{\bf{K}}_{ij}} = \kappa ({{\bf{x}}_i},{{\bf{x}}_j}) = \left\langle {\varphi ({{\bf{x}}_i}),\varphi ({{\bf{x}}_j})} \right\rangle$，则正则风险最小化问题具有如下形式：
   
$$\displaylines{
  F({\bf{\alpha }}) = \sum\limits_{j = 1}^n {({y_j} - \sum\limits_{i = 1}^n {{\alpha _i}} } \kappa ({{\bf{x}}_i},{{\bf{x}}_j}){)^2} + \lambda \sum\limits_i^n {{\alpha _i}\varphi ({{\bf{x}}_i})} \sum\limits_j^n {{\alpha _j}\varphi ({{\bf{x}}_j})}  \cr 
   = {({\bf{y - K\alpha }})^{\rm{T}}}({\bf{y - K\alpha }}) + \lambda {{\bf{\alpha }}^{\rm{T}}}{\bf{K\alpha }} \cr} $$
   欲使$F({\bf{\alpha }})$取最小值，仅需对${\bf{\alpha }}$求导，并令其等于0：

$$\displaylines{
  {{dF({\bf{\alpha }})} \over {d{\bf{\alpha }}}} = {({\bf{y - K\alpha }})^{\rm{T}}}( - {\bf{K}}{\rm{ + }}\lambda {{\bf{\alpha }}^{\rm{T}}}{\bf{K}} \cr 
  {\rm{ =  - }}{{\bf{y}}^{\rm{T}}}{\bf{K}} + {{\bf{\alpha }}^{\rm{T}}}{{\bf{K}}^{\rm{T}}}{\bf{K}} + \lambda {{\bf{\alpha }}^{\rm{T}}}{\bf{K}} \cr 
   = 0 \cr} $$
由此得到核岭回归的解为：

$$\alpha  = {({\bf{K}} + \lambda {\bf{I}})^{ - 1}}{\bf{y}}\tag{5}$$